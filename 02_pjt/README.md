# 💸 관통PJT-02 (금융)



## F01. CSV 파일 읽기 및 필드 선택



### ✅ 요구사항 내용

Kaggle에서 제공하는 넷플릭스 주가 데이터(NFLX.csv)를 Pandas를 활용해 읽어오고, 그 중에서 'Date', 'Open', 'High', 'Low', 'Close' 컬럼만 선택하여 분석에 사용할 수 있도록 전처리한다.

---

### ✏️ 학습한 내용

오늘 프로젝트의 첫 번째 단계는 넷플릭스 주가 데이터 파일을 불러오는 거였다. 처음에는 단순히 pd.read_csv()만 쓰면 끝일 줄 알았는데, 생각보다 세심하게 다뤄야 할 부분이 많았다.

CSV 파일을 불러올 때는 우선 파일 경로가 정확한지부터 확인해야 했다. 로컬에서 작업할 때는 ./data/NFLX.csv 같은 상대경로를 썼고, Jupyter Notebook에서 확인할 수 있도록 `df.head()`로 데이터를 미리 찍어보는 습관이 중요했다.

불러온 후 전체 컬럼을 `.columns`로 확인해보니 생각보다 다양한 정보들이 들어 있었는데, 이번 분석에서는 날짜와 관련된 `'Date'`, 그리고 주가의 시작가/고가/저가/종가인 `'Open'`, `'High'`, `'Low'`, `'Close'`만 사용해야 했다.

그래서 `df = df[['Date', 'Open', 'High', 'Low', 'Close']]`로 원하는 컬럼만 추출했는데, 이때 오타나 컬럼명 대소문자 실수 하나만 있어도 KeyError가 나서 주의해야 했다.

이 작업을 하면서 Pandas는 컬럼명 하나 틀리면 절대 넘어가지 않는다는 걸 다시 한 번 느꼈다.

---

### 🥲 어려웠던 점

가장 처음으로 마주한 어려움은 파일 경로 문제였다. Jupyter Notebook에서 실행하는 위치와 내가 생각하는 경로가 달라서 자꾸 `FileNotFoundError`가 났다.

또 한 가지는 CSV를 읽은 직후 전체 컬럼을 바로 확인하지 않고 분석하려고 하다가, 엉뚱한 컬럼으로 코드를 작성해서 `KeyError`가 발생한 것이다. Pandas는 컬럼명을 정확히 기억하고 써야 하는데, 'close'처럼 공백이 들어가거나 소문자로 적으면 바로 에러가 난다.

---

### ✨ 새로 배운 개념

- `pd.read_csv(filepath)`
    
    : CSV 파일을 DataFrame으로 읽어오는 기본 함수. 경로가 틀리면 에러가 발생하므로, 사전에 경로를 정확히 지정해야 한다.
    
- `df.columns`
    
    : DataFrame에 어떤 컬럼이 있는지 빠르게 확인할 수 있는 유용한 속성. 컬럼명을 그대로 복사해서 쓰면 오타 없이 코드를 작성할 수 있다.
    
- `df[['col1', 'col2', ...]]`
    
    : DataFrame에서 다수의 컬럼만 선택하고 싶을 때 사용하는 슬라이싱 문법. 리스트 형태로 넣어야 에러 없이 동작한다.
    
- `df.head(n)`
    
    : 데이터의 앞부분(n행)을 미리 확인할 수 있는 함수. 데이터가 잘 불러와졌는지, 예상한 구조인지 빠르게 체크할 수 있다.
    

---

### 💬 느낀 점

이전에도 read_csv()를 써본 적이 있었지만, 프로젝트의 출발점이 되는 만큼 작은 실수도 결과에 큰 영향을 준다는 걸 다시 실감했다. 특히 분석할 컬럼을 선택할 땐 단순히 이름만 보고 고르는 게 아니라, 그 데이터가 어떤 의미를 갖고 있는지 생각해야 한다는 점이 중요했다.

또한 Pandas에서 컬럼을 다룰 때는 한 글자, 한 공백도 민감하게 반응하니 더 꼼꼼하게 코드를 작성해야겠다고 다짐했다.

---

## F02. 2021년 이후 데이터 필터링

---

### ✅ 요구사항 내용

Pandas를 활용해 2021년 1월 1일 이후의 넷플릭스 주가 데이터만 필터링하여 분석 대상 데이터를 정제한다. 이후 분석 및 시각화는 이 필터링된 데이터셋을 기준으로 진행해야 한다.

---

### ✏️ 학습한 내용

데이터를 읽어왔으니, 이제 분석할 범위를 정해야 했다. 명세서에서는 **2021년 1월 1일 이후의 데이터만 사용**하라고 되어 있었고, 그래서 날짜를 기준으로 조건 필터링을 적용해야 했다.

처음에는 그냥 문자열 비교처럼 '2021-01-01'보다 큰 값을 필터링하면 되지 않을까 싶었는데, Pandas에서는 날짜 컬럼이 문자열이면 비교가 제대로 되지 않는 경우가 있다는 걸 깨달았다. 그래서 우선 df['Date'] 컬럼을 datetime 타입으로 변환해주는 게 필수였다.

```python
df['Date'] = pd.to_datetime(df['Date'])
```

이렇게 날짜 형식으로 변환한 후, 조건 필터링을 아래와 같이 수행했다.

```python
idxs = df[df['Date'] < pd.Timestamp('2021-01-01')].index
after_2021_df = df.drop(idxs)
```

이때 문자열인 '2021-01-01'도 자동으로 Timestamp로 해석되기 때문에 간단하게 비교가 가능했다.

필터링이 완료된 후에는 .shape, .head() 등을 이용해서 데이터 개수나 구조가 의도한 대로 잘 줄어들었는지 확인했다.

---

### 🥲 어려웠던 점

처음에는 날짜가 `object` 타입인 줄 모르고 바로 조건식을 걸었더니 이상한 결과가 나왔다.

또 하나 막혔던 건, 날짜 변환을 한 줄로 쓰지 않고 `pd.to_datetime(df['Date'])`만 호출하고, `df['Date'] = ...` 할당을 하지 않으면 원본 데이터가 바뀌지 않는다는 점이었다.

---

### ✨ 새로 배운 개념

- `pd.to_datetime(series)`
    
    : 문자열 형태의 날짜 데이터를 `datetime64[ns]` 형식으로 변환해주는 함수. 비교 연산이나 시계열 분석을 위해 반드시 필요하다.
    
- `df[조건]`
    
    : Pandas의 Boolean Indexing. 원하는 조건을 만족하는 행만 남기는 필터링 방식이다.
    
- `df['Date'] >= '2021-01-01'`
    
    : `Date` 컬럼이 `datetime64` 형식일 경우, 문자열을 비교해도 내부적으로 `Timestamp`로 변환되기 때문에 비교가 가능하다.
    

---

### 💬 느낀 점

Pandas에서는 데이터의 타입이 분석의 흐름을 좌우한다는 걸 명확히 알게 되었다. 그리고 단순히 데이터를 불러와서 쓰는 게 아니라, 분석할 범위를 스스로 지정하고 잘라내는 과정이 굉장히 중요하다는 걸 느꼈다.

이번처럼 조건을 바탕으로 데이터를 잘라내고, 그 결과를 시각적으로도 체크하는 루틴을 잘 익혀두면 앞으로 어떤 프로젝트를 하더라도 기본을 잘 다질 수 있을 것 같다.

---

## F03. 최고/최저 종가 추출

---

### ✅ 요구사항 내용

2021년 이후로 필터링된 데이터(after_2021_df)를 바탕으로, 넷플릭스 주가의 종가(Close) 중 최고값과 최저값을 각각 추출한다. 이는 전체 흐름 중 “데이터 분석” 단계의 가장 기초적인 통계 요약 작업으로, 데이터의 변동 폭을 파악하고 이후 시각화/해석의 기초 자료로 활용된다.

---

### ✏️ 학습한 내용

2021년 이후 데이터를 필터링했으니, 이제 그 안에서 종가(Close)의 최고값과 최저값을 구해야 했다.

```python
max_price = max(list(after_2021_df['Close']))
min_price = min(list(after_2021_df['Close']))

print('최고 종가:', max_price)
print('최저 종가:', min_price)
```

---

### 🥲 어려웠던 점

이번 요구사항에서는 딱히 어려운 점이 없었다.

---

### ✨ 새로 배운 개념

- 없…음…

---

### 💬 느낀 점

최고값, 최저값은 단순 통계치지만, 그 숫자 자체보다 “언제 그랬는가”가 중요하지 않을까 생각해보았다.

---

## F04. 월별 평균 종가 계산

---

### ✅ 요구사항 내용

2021년 이후로 필터링된 넷플릭스 주가 데이터를 대상으로, 월 단위로 그룹화하여 종가(Close)의 평균값을 계산하고 이를 시각화까지 수행한다. 이는 시계열 데이터를 기반으로 주기적 패턴이나 계절성을 파악하기 위한 중요한 기초 분석 과정이다.

---

### ✏️ 학습한 내용

이제 슬슬 본격적인 데이터 분석다운 작업이 시작됐다.

하루 단위로 기록된 넷플릭스 주가 데이터 중 종가(Close) 값을 월별로 묶어서 평균을 내는 것이 이번 요구사항이었다. 처음에는 `df.groupby(df['Date'].dt.month)`만 쓰면 될 줄 알았는데, 결과가 이상하게 나왔다.

왜냐면 이렇게 하면 모든 연도의 1월이 하나로, 모든 연도의 2월이 하나로 묶이기 때문이다. 즉, `'2021-01'`, `'2022-01'`이 다 섞이는 문제였다.

그래서 연도와 월을 동시에 고려하는 방식이 필요했고, 다음과 같은 방법으로 해결했다.

```python
after_2021_df['Month'] = after_2021_df['Date'].dt.strftime('%Y-%m')
group_month_after_2021_df = after_2021_df.groupby(after_2021_df['Month']).mean(numeric_only=True)
```

사실 이 방법 말고도 `dt.to_period('M')` 를 쓰는 방법을 사용했었는데, 나중에 시각화할 때 period 타입을 x축에 넣을 수 없어서 다시 str로 변형해야 했다. 그래서 고민하던 중,  환승님과 민규님이 알려준 이 방법이 가장 깔끔하여 이렇게 코드를 구성하게 되었다. Month column을 새로 생성하여 그 column 기준으로 그룹화 하는 방법이다.

이후 `matplotlib`의 `plt.plot()`으로 평균 종가(Close) 값을 선 그래프로 그려봤다.

```python
# 그래프

# 데이터 생성
x = list(group_month_after_2021_df.index) # x 좌표값
y = list(group_month_after_2021_df['Close']) # y 좌표값

# 그래프 그리기
plt.plot(x, y)

# 그래프에 제목과 축 레이블 추가
plt.title('Monthly Average Close Price')
plt.xlabel('Date')
plt.ylabel('Average Close Price')

# x축 눈금 간격 띄우기
plt.xticks(x[::2], rotation=45)

# 레이블 겹침 방지
plt.tight_layout()  
# 그래프 표시
plt.show()
```

x축 값이 많을 경우엔 `xticks`를 간격을 두고 출력하는 방식으로 정리했다.⭐⭐⭐

---

### 🥲 어려웠던 점

제일 헷갈렸던 건, groupby 대상이 Date인데, 어떻게 월별로 나눌지를 지정해야 할지였다. 처음엔 `'Date'` 컬럼을 문자열로 바꿔서 `"2021-01"`처럼 가공하려고 했는데, 문자열 비교는 시계열 연산이나 정렬에서 불편하다는 걸 깨달았다.

또 xticks를 모르는 상태에서 그래프를 그렸었는데, x축의 문자열들이 심각하게 겹치는 현상이 발생하여 애를 먹었다. 너무 서러웠지만 xticks를 활용하여 두 스텝마다 출력할 수 있도록 설정하였다.

---

### ✨ 새로 배운 개념

- `.dt.to_period('M')`
    
    : 날짜를 `'YYYY-MM'` 단위로 바꿔주는 방법. `PeriodIndex`가 생성되어 보기 좋지만, 시계열 함수 일부는 제한된다.
    
- `plt.xticks(rotation=45)`
    
    : x축 라벨이 겹치지 않도록 기울여서 출력하는 방법.
    

---

### 💬 느낀 점

날짜 데이터는 단순히 숫자처럼 다루는 게 아니라, Pandas에서 제공하는 시계열 전용 도구들을 적절히 조합해야 한다는 감각이 중요했다. 이번 요구사항을 하면서 Pandas에서 시계열 데이터를 어떻게 그룹화하고, 어떻게 시각화로 연결하는지를 배웠다.

---

## F05. 월별 고가/저가/종가 시각화

---

### ✅ 요구사항 내용

2022년 이후의 넷플릭스 주가 데이터를 필터링하여, 각 월별로 최고가(High), 최저가(Low), 종가(Close)를 시계열 선 그래프로 시각화한다. 각 가격 정보를 하나의 그래프에 함께 표현하여 시각적으로 비교할 수 있어야 하며, 범례(legend)를 통해 선들의 의미를 명확히 구분해야 한다.

---

### ✏️ 학습한 내용

먼저 2022년 이후 데이터를 필터링하기 위해 기존 DataFrame에서 날짜 조건을 다시 걸어줬다.

```python
idxs = df[df['Date'] < pd.Timestamp('2022-01-01')].index
after_2022_df = df.drop(idxs)
```

그 다음에는 이 데이터를 월 단위로 그룹화해서 `High`, `Low`, `Close` 각각의 평균을 구했다.

```python
group_month_after_2022_df = after_2022_df.groupby(df['Date']).mean(numeric_only=True)
group_month_after_2022_df
```

이제 본격적으로 그래프를 그리기 시작했는데, `plt.plot()`을 세 번 호출해서 한 그래프 안에 세 개의 선을 그려줬다.

```python
# 그래프 그리기
plt.plot(x, y1, label='High')
plt.plot(x, y2, label='Low')
plt.plot(x, y3, label='Close')

# 범례 추가
plt.legend()
```

이렇게 하면 자동으로 x축은 DatetimeIndex, y축은 각각의 주가가 된다. label을 지정한 덕분에 plt.legend()를 추가했을 때 세 선의 의미가 잘 구분되었다.

---

### 🥲 어려웠던 점

x축 눈금이 너무 많아지면 날짜가 겹쳐서 난잡하게 보이는 문제가 있었다.

처음에는 `plt.plot()`만 쓰고 끝내려다가, 그래프가 너무 복잡하게 보여서 `xticks`를 따로 설정해주는 게 필요했다.

```python
plt.xticks(rotation=45)
```

또 하나 어려웠던 건 세 선이 겹치는 구간이 많을 때 시각적으로 구분이 잘 안 되는 경우였다. 이럴 땐 색상이나 선 스타일을 다르게 해주는 것도 중요한 팁이라는 걸 배웠다.

---

### ✨ 새로 배운 개념

- `plt.plot(x, y, label=...)`
    
    : 여러 선을 한 그래프에 표현하고 범례로 구분할 수 있게 해준다.
    
- `plt.legend()`
    
    : `label`이 지정된 선들의 설명을 자동으로 추가함.
    
- `plt.xticks()`
    
    : x축 눈금을 원하는 위치에, 원하는 텍스트로 표시할 수 있게 한다. `rotation`으로 라벨 기울이기도 가능.
    

---

### 💬 느낀 점

고가, 저가, 종가가 시기마다 얼마나 엇갈리는지를 동시에 보여주는 게 흥미로웠고, 같은 데이터를 보더라도 시각화 하나로 인사이트가 확 바뀐다는 걸 체감했다. 그래프는 그리는 것보다 읽기 쉽게 정돈하는 것이 훨씬 중요하다는 걸 다시금 느낀 하루였다.

---

## F06. 생성형 AI를 활용한 주가 예측 프롬프트 구성

---

### ✅ 요구사항 내용

넷플릭스 주가 데이터를 기반으로 생성형 AI(OpenAI, Gemini 등)에게 적절한 프롬프트를 구성하고, 이를 통해 주가의 변화나 추세를 해석하거나 예측하는 결과를 도출한다. 프롬프트는 데이터 해석, 매수/매도 판단 등 분석가의 질문처럼 설계되어야 하며, AI의 응답을 통해 인사이트를 얻는 것이 목표다.

---

### ✏️ 학습한 내용

데이터를 정제하고 분석하고 시각화까지 해봤다면, 그다음은 해석과 의사결정이다. 이 요구사항은 조금 색달랐다. 직접 데이터를 분석하기보다는, 분석된 결과를 바탕으로 AI에게 질문을 던지고 의미 있는 해석을 받는 것이 핵심이었다.

처음에는 단순히 "넷플릭스 주가가 오를까요?" 같은 질문을 던졌는데, 이건 너무 막연하고 예측도 빈약했다.

그래서 시각화된 결과를 기반으로, 더 정제된 질문을 구성했다.

[예시]

![image.png](image.png)

첨부한 사진은 2022년 1월부터 2월까지의 넷플릭스 주가 변화를 나타낸 그래프입니다. 그래프를 보고 넷플릭스의 주가 동향을 분석해주세요.

---

### 🥲 어려웠던 점

어려웠던 건 없고, GPT는 최고다!

---

### ✨ 새로 배운 개념

- **프롬프트 엔지니어링**
    
    단순한 질문이 아닌, 분석 배경 + 해석 요청을 함께 담은 질문 구성이 필요하다. 예: “왜 하락했을까?”, “향후 흐름은 어떨까?”
    
- **그래프 기반 해석 질문**
    
    데이터를 직접 입력하기 어렵다면, 변화 양상을 요약한 문장을 제공하고, 이에 대해 AI가 해석하게 유도할 수 있다.
    
- **AI의 역할은 분석 보조자**
    
    AI는 데이터 자체를 보지 않기 때문에, 설명 기반 해석, 예측 가이드라인 등 보조적 조언자 역할로 활용하는 게 적합하다.
    

---

### 💬 느낀 점

처음엔 단순히 AI에게 “올라요? 떨어져요?”라고 묻는 게 전부인 줄 알았는데, 질문을 설계하는 과정이 오히려 가장 인간적이고 분석적인 작업이었다. AI가 똑똑해서 좋은 게 아니라, 내가 제대로 요약하고 질문을 던졌을 때만 비로소 그 똑똑함이 빛을 발한다는 걸 배웠다. 이번 과정을 통해 느낀 건, 데이터를 해석하는 힘은 결국 사람의 손에서 시작되고, AI는 그 흐름을 보완하는 수단이라는 점이다. 앞으로 실무에서 생성형 AI를 쓸 기회가 온다면, 단순히 "무엇을 대신해주는 도구"가 아니라, 나의 해석을 더 정밀하게 다듬어줄 파트너로 대할 수 있을 것 같다.

---